{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE this part: import libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json valid.json 20\n"
     ]
    }
   ],
   "source": [
    "# DON'T CHANGE this part: read data path\n",
    "train_set_path, valid_set_path, random_number = input().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
    "2. embedding: hitogram matrix\n",
    "3. classifier using linear regression\n",
    "4. accuracy (for metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(train_set_path,valid_set_path):\n",
    "    with open(train_set_path,\"r\") as file_train:\n",
    "        training_data=json.load(file_train)\n",
    "    file_train.close()\n",
    "\n",
    "    with open(valid_set_path,\"r\") as file_valid:\n",
    "        valid_data=json.load(file_valid)\n",
    "    file_valid.close()\n",
    "    return training_data,valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ví dụ cho phần báo cáo, nên báo cáo cho từng phần code để rõ ràng\n",
    "\n",
    "Báo cáo phần tiền xử lý: dùng xyz để tách từ, ...\n",
    "\n",
    "... Đối với những từ out-of-vocab (xuất hiện trong tập train nhưng không có ở tập valid), xử lý bằng cách ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ cho phần code hàm tiền xử lý\n",
    "\n",
    "def preprocess(text):\n",
    "    # converting text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #text=re.sub(\"[^A-Za-z0-9]\",\" \",text)\n",
    "    text=re.sub(\"[+-]?(\\d+(\\.\\d+)?|\\.\\d+)([eE][+-]?\\d+)?\",\"num\",text)\n",
    "    \"\"\"\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            text=re.sub(word,\"num\",text)\n",
    "    \"\"\"\n",
    "    words=word_tokenize(text)\n",
    "    \n",
    "    stopword=stopwords.words(\"english\")\n",
    "    tmp=[word for word in words if word not in stopword]\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    words=[stemmer.stem(word) for word in tmp]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_training_data(training_data):\n",
    "    preprocessed_text=list()\n",
    "    labels=list()\n",
    "    for data in training_data:\n",
    "        preprocessed_text.append(preprocess(data[\"reviewText\"]))\n",
    "        labels.append(data[\"overall\"])\n",
    "    return preprocessed_text,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def buildGrossary(training_preprocessed_text):\n",
    "    vocabulary=list()\n",
    "    for preprocessed_text in training_preprocessed_text:\n",
    "        for word in preprocessed_text:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "    vocabulary.append(\"unk\")\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_valid_data(valid_data,vocabulary):\n",
    "    preprocessed_text=list()\n",
    "    labels=list()\n",
    "    for data in valid_data:\n",
    "        tmp_preprocessed=preprocess(data[\"reviewText\"])\n",
    "        tmp=[word if word in vocabulary else \"unk\" for word in tmp_preprocessed]\n",
    "        preprocessed_text.append(tmp)\n",
    "        labels.append(data[\"overall\"])\n",
    "    return preprocessed_text,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_histogram_matrix(preprocessed_text,vocabulary):\n",
    "    A=list()\n",
    "    for data in preprocessed_text:\n",
    "        word_count=np.zeros(len(vocabulary))\n",
    "        for idx,word in enumerate(vocabulary):\n",
    "            if word in data:\n",
    "                word_count[idx]=data.count(word)\n",
    "        word_count=np.insert(word_count,0,1)\n",
    "        A.append(word_count)\n",
    "    A=np.array(A)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2_classifier(A,training_labels):\n",
    "    b=list()\n",
    "    for label in training_labels:\n",
    "        tmp=np.zeros(5)\n",
    "        if label==1.0:\n",
    "            tmp[0]=1\n",
    "        elif label==2.0:\n",
    "            tmp[1]=1\n",
    "        elif label==3.0:\n",
    "            tmp[2]=1\n",
    "        elif label==4.0:\n",
    "            tmp[3]=1\n",
    "        elif label==5.0:\n",
    "            tmp[4]=1\n",
    "        b.append(tmp)\n",
    "    b=np.array(b)\n",
    "    x_hat=np.linalg.pinv(A)@b\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,valid_labels):\n",
    "    dem=0\n",
    "    for y,label in zip(y_hat,valid_labels):\n",
    "        newOverall=float(np.argmax(scipy.special.softmax(y))+1)\n",
    "        #print(newOverall, label)\n",
    "        if newOverall==label:\n",
    "            dem+=1\n",
    "        acc=dem/len(valid_labels)\n",
    "    return acc\n",
    "#print(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,valid_data=loadData(train_set_path,valid_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "y_hat=document@x_hat+assess\n",
    "dem=0\n",
    "for y,data in zip(y_hat,valid_data):\n",
    "    newy=softmax(y)\n",
    "    newOverall=float(np.argmax(newy)+1)\n",
    "    #print(newOverall, data[\"overall\"])\n",
    "    if newOverall==data[\"overall\"]:\n",
    "        dem+=1\n",
    "acc=dem/len(training_data)\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preprocessed_text,training_labels=preprocessed_training_data(training_data)\n",
    "vocabulary=buildGrossary(training_preprocessed_text)\n",
    "A_training=embedding_histogram_matrix(training_preprocessed_text,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preprocessed_text,valid_labels=preprocess_valid_data(valid_data,vocabulary)\n",
    "A_valid=embedding_histogram_matrix(valid_preprocessed_text,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06301719  0.01198465  0.05852005  0.11724848  0.74922963]\n",
      " [ 0.00799906 -0.00490158 -0.01831092 -0.05979875  0.0750122 ]\n",
      " [ 0.18399913  0.03180876 -0.00252505  0.43373081 -0.64701364]\n",
      " ...\n",
      " [ 0.01106412  1.45520738  1.88683551 -2.14162697 -1.21148004]\n",
      " [-0.38061365 -0.08851808  0.20372067  0.85105113 -0.58564007]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "x_hat=M2_classifier(A_training,training_labels)\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=accuracy(A_valid@x_hat,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', ',', 'hold', 'save', '.', 'still', 'abl', 'read', 'label', '!', 'would', 'purchas', ',', 'recommend', '.']\n",
      "M2 - 0.502\n"
     ]
    }
   ],
   "source": [
    "print(valid_preprocessed_text[int(random_number)])\n",
    "print(\"M2 -\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
